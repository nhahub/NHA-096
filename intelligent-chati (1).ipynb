{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13855975,"sourceType":"datasetVersion","datasetId":8826735}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Attention Mechanism\n---","metadata":{}},{"cell_type":"markdown","source":"# Imports & Device Setup","metadata":{}},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Model & Tokenizer","metadata":{}},{"cell_type":"code","source":"MODEL_PATH = \"/kaggle/input/basic-model/saved_model/saved_gpt2_chatbot\"\n\ntokenizer = AutoModelForCausalLM.from_pretrained(MODEL_PATH)._tokenizer if False else AutoTokenizer.from_pretrained(MODEL_PATH)\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_PATH,\n    output_attentions=True\n).to(device)\n\nmodel.eval()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Attention Inspection","metadata":{}},{"cell_type":"code","source":"def inspect_attention(text):\n    \"\"\"\n    Runs a forward pass through the model and visualizes attention \n    heatmap for Layer 1, Head 1.\n\n    Args:\n        text (str): Input sentence to inspect.\n    \"\"\"\n    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    attentions = outputs.attentions\n    print(f\"Number of layers: {len(attentions)}\")\n    print(f\"Shape of first layer attention: {attentions[0].shape}\")\n\n    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n    attn_matrix = attentions[0][0, 0].cpu().numpy()\n\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(attn_matrix, xticklabels=tokens, yticklabels=tokens, cmap=\"viridis\")\n    plt.title(\"Attention Heatmap - Layer 1, Head 1\")\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inspect_attention(\"Hi, how are you?\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Generate Reply","metadata":{}},{"cell_type":"code","source":"def generate_reply(history):\n    \"\"\"\n    Generates a chatbot reply based on the entire conversation history.\n\n    Args:\n        history (str): Full conversation so far.\n\n    Returns:\n        str: Generated bot reply.\n    \"\"\"\n    inputs = tokenizer(history, return_tensors=\"pt\").to(device)\n\n    with torch.no_grad():\n        output = model.generate(\n            **inputs,\n            max_new_tokens=80,\n            temperature=0.7,\n            top_p=0.9,\n            repetition_penalty=1.4,\n            pad_token_id=tokenizer.eos_token_id\n        )\n\n    return tokenizer.decode(output[0], skip_special_tokens=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Chatbot Loop","metadata":{}},{"cell_type":"code","source":"def run_chatbot():\n    \"\"\"\n    Runs an interactive multi-turn chatbot loop.\n    Keeps adding messages to a conversation buffer and \n    generates context-aware responses.\n    \"\"\"\n    print(\"\\n=== Multi-Turn Chatbot ===\\n\")\n\n    conversation_history = \"\"\n\n    while True:\n        user_msg = input(\"User: \").strip()\n        if user_msg.lower() == \"exit\":\n            break\n\n        conversation_history += f\"User: {user_msg}\\nBot:\"\n\n        full_output = generate_reply(conversation_history)\n        bot_reply = full_output.split(\"Bot:\")[-1].strip()\n\n        print(f\"Bot: {bot_reply}\")\n        print(\"-\" * 50)\n\n        conversation_history += f\" {bot_reply}\\n\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"run_chatbot()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"-----","metadata":{}},{"cell_type":"markdown","source":"# Continous Learning\n---","metadata":{}},{"cell_type":"markdown","source":"# Imports & Model Loading","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport torch\nfrom transformers import (\n    GPT2LMHeadModel,\n    GPT2Tokenizer,\n    Trainer,\n    TrainingArguments,\n    TextDataset,\n    DataCollatorForLanguageModeling,\n)\n\nMODEL_PATH = \"/kaggle/input/basic-model/saved_model/saved_gpt2_chatbot\"\nDATA_PATH = \"new_data.json\"\n\n# Load tokenizer & model\ntokenizer = GPT2Tokenizer.from_pretrained(MODEL_PATH)\nmodel = GPT2LMHeadModel.from_pretrained(MODEL_PATH)\n\n# Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval()\n\nprint(f\"Model running on: {device}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Chatbot Inference","metadata":{}},{"cell_type":"code","source":"def chat_with_bot(user_input, max_length=150):\n    \"\"\"\n    Generate a reply from the chatbot using GPT-2 model.\n    \"\"\"\n    formatted_input = f\"<user>{user_input}<bot>\"\n    input_ids = tokenizer.encode(formatted_input, return_tensors=\"pt\").to(device)\n\n    with torch.no_grad():\n        output_ids = model.generate(\n            input_ids,\n            max_length=max_length,\n            pad_token_id=tokenizer.eos_token_id,\n            do_sample=True,\n            top_k=50,\n            top_p=0.95,\n        )\n\n    decoded_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n    bot_reply = decoded_text.split(\"<bot>\")[-1].strip()\n\n    return bot_reply","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Saving","metadata":{}},{"cell_type":"code","source":"def add_new_interaction(user_text, bot_text, feedback):\n    \"\"\"\n    Save user-bot interaction + feedback to dataset file.\n    \"\"\"\n    interaction = {\n        \"user\": f\"<user>{user_text}\",\n        \"bot\": f\"<bot>{bot_text}\",\n        \"feedback\": feedback,\n    }\n\n    if not os.path.exists(DATA_PATH):\n        with open(DATA_PATH, \"w\") as f:\n            json.dump([], f)\n\n    with open(DATA_PATH, \"r+\") as f:\n        data = json.load(f)\n        data.append(interaction)\n        f.seek(0)\n        json.dump(data, f, indent=2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_good_interactions(min_feedback=4):\n    \"\"\"\n    Load all interactions that have feedback >= min_feedback.\n    \"\"\"\n    if not os.path.exists(DATA_PATH):\n        return []\n\n    with open(DATA_PATH, \"r\") as f:\n        data = json.load(f)\n\n    return [\n        f\"{item['user']}{item['bot']}\"\n        for item in data\n        if item[\"feedback\"] >= min_feedback\n    ]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_training_file(pairs, file_path=\"temp_finetune.txt\"):\n    \"\"\"\n    Write selected pairs into a text file for GPT-2 fine-tuning.\n    \"\"\"\n    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n        for line in pairs:\n            f.write(line + \"\\n\")\n    return file_path","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Fine-Tuning","metadata":{}},{"cell_type":"code","source":"def fine_tune_on_new_data(train_file, output_dir=\"./updated_model\"):\n    \"\"\"\n    Fine-tune GPT-2 model on collected user-bot interactions.\n    \"\"\"\n    dataset = TextDataset(\n        tokenizer=tokenizer,\n        file_path=train_file,\n        block_size=128,\n    )\n\n    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        overwrite_output_dir=True,\n        num_train_epochs=3,\n        per_device_train_batch_size=2,\n        save_total_limit=1,\n        logging_steps=20,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        data_collator=data_collator,\n        train_dataset=dataset,\n    )\n\n    trainer.train()\n    trainer.save_model(output_dir)\n\n    print(\"Model updated successfully!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def check_and_train(threshold=200):\n    \"\"\"\n    Check if enough high-quality feedback exists → retrain the model.\n    \"\"\"\n    good_pairs = load_good_interactions()\n\n    if len(good_pairs) >= threshold:\n        print(f\"Training triggered with {len(good_pairs)} high-quality examples!\")\n\n        train_file = create_training_file(good_pairs)\n        fine_tune_on_new_data(train_file)\n\n        # Clear file after successful training\n        with open(DATA_PATH, \"w\") as f:\n            json.dump([], f)\n\n        print(\"Training complete. Data cleared!\")\n    else:\n        print(\n            f\"Collected {len(good_pairs)} good examples. \"\n            f\"{threshold - len(good_pairs)} more needed.\"\n        )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Chat Loop","metadata":{}},{"cell_type":"code","source":"def run_chat(threshold=50):\n    \"\"\"\n    Continuous chat loop + feedback collection + auto-training trigger.\n    \"\"\"\n    print(\"Welcome! Type 'exit' to quit the chat.\")\n\n    while True:\n        user_input = input(\"You: \").strip()\n\n        if user_input.lower() == \"exit\":\n            print(\"Chat ended.\")\n            break\n\n        # Generate bot response\n        bot_reply = chat_with_bot(user_input)\n        print(\"Bot:\", bot_reply)\n\n        # Collect feedback\n        while True:\n            try:\n                feedback = int(input(\"Rate the bot reply (1–5): \"))\n                if 1 <= feedback <= 5:\n                    break\n                print(\"Rating must be between 1 and 5.\")\n            except:\n                print(\"Please enter a valid number.\")\n\n        # Save interaction + check for training\n        add_new_interaction(user_input, bot_reply, feedback)\n        check_and_train(threshold=threshold)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"run_chat(threshold=50)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}}]}