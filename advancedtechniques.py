# -*- coding: utf-8 -*-
"""advancedtechniques.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BojAt1tErUYa6HGrbOt99jUrgVv1o3Im
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

!pip install -q --no-cache-dir torchvision==0.18.1
!pip install -q --no-cache-dir transformers==4.44.2 accelerate

# Runtime → Restart Runtime

import os
os.kill(os.getpid(), 9)

#!pip install -q transformers datasets accelerate sentence-transformers faiss-cpu torch tqdm --upgrade
import os
import json
import torch
import time
import logging
from typing import List, Dict
from torch.utils.data import Dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ChatbotConfig:
    """Configuration class for easy parameterization."""
    def __init__(self):
        self.model_name = '/kaggle/input/basemodel/saved_gpt2_chatbot'
        self.finetuned_model_dir = './enhanced_chatbot_model'
        self.feedback_file = 'feedback.json'
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.embed_model = 'sentence-transformers/all-MiniLM-L6-v2'
        self.retrieve_k = 4
        self.max_history_turns = 50
        self.max_history_tokens = 1024
        self.min_new_positive = 5  # For fine-tuning trigger
        self.generation_params = {
            'temperature': 0.7,
            'top_p': 0.9,
            'max_new_tokens': 128
        }

# ------------------ Feedback Utils ------------------
def load_feedback(path: str) -> List[Dict]:
    if os.path.exists(path):
        try:
            with open(path, 'r') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"Error loading feedback: {e}")
            return []
    return []

def save_feedback(history: List[Dict], path: str):
    try:
        with open(path, 'w') as f:
            json.dump(history, f, indent=2, ensure_ascii=False)
    except Exception as e:
        logger.error(f"Error saving feedback: {e}")



# ------------------ Model Utils ------------------
def load_model_and_tokenizer(config: ChatbotConfig):
    from transformers import GPT2Tokenizer, AutoModelForCausalLM  # استخدم GPT2Tokenizer بدل AutoTokenizer

    tokenizer = GPT2Tokenizer.from_pretrained(config.model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(config.model_name).to(config.device)
    model.config.pad_token_id = tokenizer.eos_token_id

    # Load fine-tuned if exists
    if os.path.exists(config.finetuned_model_dir):
        try:
            model = AutoModelForCausalLM.from_pretrained(config.finetuned_model_dir).to(config.device)
            tokenizer = GPT2Tokenizer.from_pretrained(config.finetuned_model_dir)  # نفس الشيء هنا
            logger.info(f"Loaded fine-tuned model from {config.finetuned_model_dir}")
        except Exception as e:
            logger.warning(f"Could not load fine-tuned model: {e}")
    return model, tokenizer

# ------------------ Retrieval (FAISS + SBERT) ------------------
try:
    from sentence_transformers import SentenceTransformer
    import numpy as np
    import faiss
except ImportError:
    SentenceTransformer = None
    faiss = None

class RetrievalSystem:
    def __init__(self, config: ChatbotConfig):
        self.sbert = SentenceTransformer(config.embed_model) if SentenceTransformer else None
        self._faiss_index = None
        self._faiss_texts: List[str] = []
        self.retrieve_k = config.retrieve_k

    def build_faiss_index(self, feedback_list: List[Dict]):
        if self.sbert is None:
            return
        texts = [f"USER: {h['user']} BOT: {h['bot']}" for h in feedback_list]
        if not texts:
            self._faiss_index = None
            self._faiss_texts = []
            return
        embeddings = self.sbert.encode(texts, convert_to_numpy=True)
        dim = embeddings.shape[1]
        idx = faiss.IndexFlatL2(dim)
        idx.add(embeddings)
        self._faiss_index = idx
        self._faiss_texts = texts

    def retrieve_similar_context(self, query: str) -> List[str]:
        if self.sbert is None or self._faiss_index is None:
            return []
        q_emb = self.sbert.encode([query], convert_to_numpy=True)
        D, I = self._faiss_index.search(q_emb, self.retrieve_k)
        results = [self._faiss_texts[i] for i in I[0] if i < len(self._faiss_texts)]
        return results

# ------------------ Conversation Memory ------------------
class ConversationMemory:
    def __init__(self, tokenizer, config: ChatbotConfig):
        self.tokenizer = tokenizer
        self.max_tokens = config.max_history_tokens
        self.max_turns = config.max_history_turns
        self.turns: List[Dict[str, str]] = []

    def add_turn(self, user: str, bot: str):
        self.turns.append({"user": user, "bot": bot})
        if len(self.turns) > self.max_turns:
            self.turns = self.turns[-self.max_turns:]

    def get_full_history(self) -> str:
        return "\n".join([f"<usr> {t['user']} <bot> {t['bot']}" for t in self.turns])

    def get_tokenized_history(self):
        text = self.get_full_history()
        return self.tokenizer(text, return_tensors="pt", truncation=True, max_length=self.max_tokens, padding='max_length')

# ------------------ Response Generation ------------------
def generate_response(user_input: str, model, tokenizer, retrieval: RetrievalSystem, conv_memory: ConversationMemory, config: ChatbotConfig, use_retrieval=True) -> str:
    retrieved = retrieval.retrieve_similar_context(user_input) if use_retrieval else []
    retrieved_section = '\n'.join([f"<retrieved> {r}" for r in retrieved]) + '\n' if retrieved else ''
    prompt = retrieved_section + conv_memory.get_full_history() + f"\n<usr> {user_input} <bot>"
    inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=config.max_history_tokens).to(config.device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=config.generation_params['max_new_tokens'],
        do_sample=True,
        top_p=config.generation_params['top_p'],
        temperature=config.generation_params['temperature'],
        pad_token_id=tokenizer.eos_token_id
    )
    gen_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    reply = gen_text.split('<bot>')[-1].strip() if '<bot>' in gen_text else gen_text[len(tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)):].strip()
    conv_memory.add_turn(user_input, reply)
    return reply

# ------------------ Feedback Fine-tuning ------------------
class FeedbackDataset(Dataset):
    def __init__(self, examples: List[str], tokenizer, max_len=512):
        self.examples = examples
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, idx):
        txt = self.examples[idx]
        tok = self.tokenizer(txt, truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt')
        return {k: v.squeeze(0) for k, v in tok.items()}

def prepare_supervised_examples_from_feedback(feedback_list: List[Dict], min_score=4) -> List[str]:
    return [f"<usr> {h['user']} <bot> {h['bot']} <|endoftext|>" for h in feedback_list if h.get('score', 0) >= min_score]

def fine_tune_on_feedback(examples: List[str], model, tokenizer, config: ChatbotConfig):
    if not examples:
        logger.info('No positive feedback examples to fine-tune on.')
        return
    ds = FeedbackDataset(examples, tokenizer, max_len=256)
    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
    training_args = TrainingArguments(
        output_dir=config.finetuned_model_dir,
        num_train_epochs=2,
        per_device_train_batch_size=2,
        learning_rate=5e-5,
        logging_steps=10,
        save_steps=200,
        save_total_limit=2,
        overwrite_output_dir=True
    )
    trainer = Trainer(model=model, args=training_args, train_dataset=ds, data_collator=data_collator)
    trainer.train()
    trainer.save_model(config.finetuned_model_dir)
    logger.info(f'Fine-tuning complete and saved to {config.finetuned_model_dir}')

# ------------------ Positive Feedback Tracking ------------------
class FeedbackTracker:
    def __init__(self, config: ChatbotConfig):
        self.config = config
        self._last_known_positive_count = len([h for h in load_feedback(config.feedback_file) if h.get('score', 0) >= 4])

    def maybe_finetune(self, model, tokenizer, retrieval: RetrievalSystem):
        fb = load_feedback(self.config.feedback_file)
        positive_feedback = [h for h in fb if h.get('score', 0) >= 4]
        new_positive_count = len(positive_feedback) - self._last_known_positive_count
        if new_positive_count >= self.config.min_new_positive:
            examples = prepare_supervised_examples_from_feedback(fb)
            if examples:
                logger.info(f'Detected {new_positive_count} new positive feedbacks -> starting fine-tune')
                fine_tune_on_feedback(examples, model, tokenizer, self.config)
                retrieval.build_faiss_index(fb)
                self._last_known_positive_count = len(positive_feedback)
            else:
                logger.info('New positive feedbacks exist but no examples prepared.')

# ------------------ Main CLI ------------------
if __name__ == '__main__':
    config = ChatbotConfig()
    model, tokenizer = load_model_and_tokenizer(config)
    retrieval = RetrievalSystem(config)
    retrieval.build_faiss_index(load_feedback(config.feedback_file))
    conv_memory = ConversationMemory(tokenizer, config)
    tracker = FeedbackTracker(config)

    print('\nAdvanced Enhanced Chatbot ready. Type "exit" to quit.')
    while True:
        try:
            user = input('\nYou: ')
        except EOFError:
            break
        if not user:
            continue
        if user.strip().lower() in ['exit', 'quit']:
            break
        resp = generate_response(user, model, tokenizer, retrieval, conv_memory, config)
        print('\nBot:', resp)
        while True:
            rating = input('Rate 1-5 (or Enter to skip): ').strip()
            if rating == '':
                break
            if rating.isdigit() and 1 <= int(rating) <= 5:
                rating = int(rating)
                fb = load_feedback(config.feedback_file)
                fb.append({'user': user, 'bot': resp, 'score': rating})
                save_feedback(fb, config.feedback_file)
                print(f'Feedback saved: {rating}/5')
                retrieval.build_faiss_index(fb)
                tracker.maybe_finetune(model, tokenizer, retrieval)
                break
            else:
                print('Invalid rating! Enter 1-5 or skip.')
    print('Session ended')

